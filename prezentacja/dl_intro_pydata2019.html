<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Introduction to Deep Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="PyData 2019" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/idash.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Deep Learning
### PyData 2019

---





background-image: url('www/img/logo_black.png')
background-size: 50%
class: center, bottom

##&lt;a href = "https://idash.pl/" target = "blank"&gt;idash.pl&lt;/a&gt;

---
class: middle, center

# The philosophy of iDash

---
## Training topics

- __M__achine __L__earning (_R, Python_),
--

- __D__eep __L__earning (_R, Python_),
--

- data processing (_R, Python_),
--

- introductions (_R, Python_),

--

and many more. Core programmes are available on &lt;a href = "https://idash.pl/en" target = "blank"&gt;our website&lt;/a&gt;.


---
class: inverse, center, middle
# Trainers

---
class: inverse, center, middle
# Rules

---
## Data

We're working on a publicly available dataset containing the features of the __Airbnb properties__ in London along with their prices per night. We've slightly preprocessed the data for the purpose of this training.

The original data can be found &lt;a href = "http://insideairbnb.com" target = "_blank"&gt;here&lt;/a&gt;.

---
## Plan for today

--

- What are __Deep Learning__ and __Neural Networks__

--

- The __components__ of a neural network

--

- What is __Keras__

--

- How to __implement__ a simple neural network __predicting the price of a property based on it's features__.

---
## Technicalities

&lt;center&gt;
&lt;br&gt;
This &lt;b&gt;slidedeck&lt;/b&gt; is available at:

&lt;a href="http://bit.do/fkG9u"&gt;&lt;h2&gt;bit.do/fkG9u&lt;/h2&gt;&lt;/a&gt;
&lt;/center&gt;

--

&lt;center&gt;
&lt;br&gt;
The starting point is this Google Colab &lt;b&gt;notebook&lt;/b&gt;:

&lt;a href="http://bit.do/fkxFk"&gt;&lt;h2&gt;bit.do/fkxFk&lt;/h2&gt;&lt;/a&gt;
&lt;/center&gt;

---
## Google Colab - copying the notebook

&lt;center&gt;
&lt;img src='www/img/colab.gif' width = "40%" style = "margin-top:30px"/&gt;
&lt;/center&gt;

---
class: inverse, center, middle

# Let's get to work!

---
## What is a neural network

Neural network is just a complex __function__.

--

Function, is a construct that returns __specific output__ given __specific input__ (arguments).

---
## The structure of a neural network

.center[
&lt;img src='www/img/network_structure.png' width = "80%"/&gt;
]

---
## Layer

.pull-left[
&lt;img src='www/img/Warstwa.png' width = "80%"/&gt;
]

.pull-right[
Layer is made of single __neurons__.

Neural network can have __many layers__.
]

---
## Neuron 

.pull-left[
&lt;img src='www/img/Neuron.png' width = "80%"/&gt;
]

.pull-right[
Neuron is __the smallest element__ of the neural network.
]

---
## Neuron 

.pull-left[
&lt;img src='www/img/Neuron.png' width = "80%"/&gt;
]

.pull-right[
Neuron is __the smallest element__ of the neural network.

In the simplest possible scenario, it's a multi variable __linear function__.
]

---
## Neuron

.pull-left[
&lt;img src='www/img/Neuron_input.png' width = "80%"/&gt;
]

.pull-right[
This function takes values from the previous layer, multiplies it by some __weights__ and adds the results together.
]

---
## Neuron

.pull-left[
&lt;img src='www/img/Neuron_input_annotated.png' width = "80%"/&gt;
]

.pull-right[
This function takes values from the previous layer, multiplies it by some __weights__ and adds the results together.

Calculation for a single neuron looks like this:

`\(y = w_1 * x_1 + w_2 * x_2 + b\)`
]

---
## Neuron

.pull-left[
&lt;img src='www/img/Neuron_input_annotated.png' width = "80%"/&gt;
]

.pull-right[
This function takes values from the previous layer, multiplies it by some __weights__ and adds the results together.

Calculation for a single neuron looks like this:

`\(y = w_1 * x_1 + w_2 * x_2 + b\)`

__Question:__ What's the number of parameters of the selected (blue) neuron? 
]

---

## Neuron 

.pull-left[
&lt;img src='www/img/Neuron_output.png' width = "80%"/&gt;
]

.pull-right[
The calculated value is then used as the input for the neurons in the __next layer__.
]

---

## Neuron 

.pull-left[
&lt;img src='www/img/Neuron_output.png' width = "80%"/&gt;
]

.pull-right[
The calculated value is then used as the input for the neurons in the __next layer__.

Therefore, the neural network (in the simplest scenario) is made of __multiple connected linear functions__.
]

---
## How does it all work?

--

The __goal of the network__ is to __find weights__ for all the parameters so that the result of executing all those connected functions is as close to original data as possible.

--

__Training the network__, is therefore the process of adjusting those weights.

---
class: inverse, center, middle

# Deep Learning

---
## What is Deep Learning?

.pull-left[
We call using __complex neural networks__ with __many layers__ "Deep Learning".

Deep learning methods are therefore still __complex functions__!
]

.pull-right[
&lt;img src='www/img/Warstwy.png' width = "80%"/&gt;
]


---
class: inverse, center, middle

# Why Deep Learning

---
## Why Deep Learning

&lt;center&gt;
&lt;img src='www/img/dl_vs_regular_eng.png' width = "90%"/&gt;
&lt;/center&gt;

???

Analogia do budowania rakiety.

---
## Applications of Deep Learning

.pull-left.medium-text[
Deep Learning methods can be used to solve __many types of problems__:
]

.pull-right[
&lt;img src='www/img/mnist_sample.png' width = "70%"/&gt;
]

---
## Applications of Deep Learning

.pull-left.medium-text[
Deep Learning methods can be used to solve __many types of problems__:

- classification of images and sound,
]

.pull-right[
&lt;img src='www/img/mnist_sample.png' width = "70%"/&gt;
]

---
## Applications of Deep Learning

.pull-left.medium-text[
Deep Learning methods can be used to solve __many types of problems__:

- classification of images and sound,
- building complex autonomous systems,
]

.pull-right[
&lt;img src='www/img/mnist_sample.png' width = "70%"/&gt;
]

---
## Applications of Deep Learning

.pull-left.medium-text[
Deep Learning methods can be used to solve __many types of problems__:

- classification of images and sound,
- building complex autonomous systems,
- everyday problems (like classification and regression on tabular data).
]

.pull-right[
&lt;img src='www/img/mnist_sample.png' width = "70%"/&gt;
]

---
## Applications of Deep Learning

.pull-left.medium-text[
Deep Learning methods can be used to solve __many types of problems__:

- classification of images and sound,
- building complex autonomous systems,
- everyday problems (like classification and regression on tabular data).

However, Deep Learning solutions work best on unstructured data.
]

.pull-right[
&lt;img src='www/img/mnist_sample.png' width = "70%"/&gt;
]

---

## Machine Learning vs. Deep Learning

&lt;center&gt;
&lt;img src='www/img/Deep_Learning_eng.png' width = "50%" style = "margin-top:30px"/&gt;
&lt;/center&gt;

---
class: inverse, center, middle

# How to build neural networks?

---
class: inverse, center, middle

# With Keras!

---
## What is Keras?

&lt;div class="keras-header"&gt;&lt;/div&gt;

__Keras__ is a high level library allowing to build neural networks.

--

The ultimate goal of Keras is to provide an __easy to use set of commands__ allowing to build networks quickly.

---
## Working with Keras

&lt;div class="keras-header"&gt;&lt;/div&gt;

The typical process of creating and using Neural Networks can be divided into the following stages:

--

1. Defining the structure of the network

--

2. Defining the way of training

--

3. Training

--

4. Evaluation

--

5. Prediction

---
class: middle, center
&lt;img src='www/img/nn_workflow_1_eng.png' width = "100%"/&gt;

---
class: inverse, center, middle

# &lt;a href = "https://colab.research.google.com/drive/1zrx5fcpYrF1iJpQLxQGFvWgvnLRy94IT#scrollTo=FmRD70BDbOBg" target = "blank"&gt;DEMO&lt;/a&gt;

???

https://colab.research.google.com/drive/1ltaGeE-uYSFpO1bXApd1FJIzJf5Qxfdk

---
## Initializing the model

&lt;div class="keras-header"&gt;&lt;/div&gt;

First thing we need to do to define the network structure is to __initialize the model__. This can be achieved using the `Sequential` function from `keras.models` module.


```python
import tensorflow.keras as keras
model = keras.models.Sequential()
```

---
## Defining the network structure

&lt;div class="keras-header"&gt;&lt;/div&gt;

Layers can be added by executing the `add()` function __on the model object__ created in the previous step.


```python
model.add(keras.layers.Dense(units = 4, input_dim = 2))
model.add(keras.layers.Dense(units = 1))
```

--

The most important parameter is the `units` parameter which defines the size of a layer.

--

It's also very important to define __number of input variables__ in the first call to the `add()` function. This can be done using the `input_dim` parameter.

---
class: inverse, center, middle

# Ok, but what is "dense"?

---
## Defining the network structure

&lt;div class="keras-header"&gt;&lt;/div&gt;

.pull-left[
&lt;img src='www/img/Input_dense_keras.png' width = "80%"/&gt;
]

.pull-right[

.small-code[

```python
import tensorflow.keras as keras
model = keras.models.Sequential()

*model.add(
* keras.layers.Dense(
*   units = 4,
*   input_dim = 2
* )
*)
model.add(
  keras.layers.Dense(units = 4)
)
model.add(
  keras.layers.Dense(units = 1)
)
```
]]
---
## Defining the network structure

&lt;div class="keras-header"&gt;&lt;/div&gt;

.pull-left[
&lt;img src='www/img/dense_keras.png' width = "80%"/&gt;
]

.pull-right[

.small-code[

```python
import tensorflow.keras as keras
model = keras.models.Sequential()

model.add(
  keras.layers.Dense(
    units = 4,
    input_dim = 2
  )
)
*model.add(
* keras.layers.Dense(units = 4)
*)
model.add(
  keras.layers.Dense(units = 1)
)
```
]]
---

## Defining the network structure

&lt;div class="keras-header"&gt;&lt;/div&gt;

.pull-left[
&lt;img src='www/img/output_keras.png' width = "80%"/&gt;
]

.pull-right[

.small-code[

```python
import tensorflow.keras as keras
model = keras.models.Sequential()

model.add(
  keras.layers.Dense(
    units = 4,
    input_dim = 2
  )
)
model.add(
  keras.layers.Dense(units = 4)
)
*model.add(
* keras.layers.Dense(units = 1)
*)
```
]]
---

## Defining the network structure

&lt;div class="keras-header"&gt;&lt;/div&gt;

The __current structure__ of the network can be looked up by calling the `summary()` method on the model object. 

--

.super-small-code[

```r
model.summary()
```
]

--

.super-small-code[

```r
_________________________________________________________________
Layer (type)                 Output Shape              Param   
=================================================================
first_layer (Dense)          (None, 4)                 12        
_________________________________________________________________
second_layer (Dense)         (None, 4)                 20        
_________________________________________________________________
output_layer (Dense)         (None, 1)                 5         
=================================================================
Total params: 37
Trainable params: 37
Non-trainable params: 0
```
]

---
class: inverse

### Exercise 1 (10 min)

1.&amp;nbsp;Execute the code located in the template notebook. It will load the data for the properties in London. __Get acquainted with the data__.

--

2.&amp;nbsp;Create the network structure that:

--

- takes 2 input variables,
--

- has one hidden layer of size 256,
--

- returns a single value.

--

Print the network structure. How many parameters are in the entire network?

---
class: middle, center
&lt;img src='www/img/nn_workflow_2_eng.png' width = "100%"/&gt;

---
class: inverse, middle, center

# The way the network is trained

---
## Training the network

To be able to train the network we need to define 2 elements: the __loss function__ and the __optimizer__ algorithm.

--

The loss function calculates the __difference__ between the __output__ of the network and the __real values__ present in the data.

--

The optimizer, given the loss function is able to __update__ the parameters so that the __value of the loss function will be minimized__.

--

One of the most popular optimization algorithms is `Adam`.

--

More details about this and other optimizers can be found &lt;a href = "https://keras.io/optimizers/" target = "_blank"&gt;here&lt;/a&gt;.

---
class: inverse, middle, center

# Mean Squared Error

---
## Training the network

Training the network is an __iterative process__:
--

1. The __data__ and __randomly initialized weights__ allow to calculate the value of the output and therefore the value of the loss function.

--

2. The optimizer updates the weights.

--

3. The updated weights and __the same data__ allow to calculate the value of the loss function again.

--

4. Steps 2-3 are repeated.

---
class: inverse, middle, center

# How to do this in Keras?

---
## Defining the way the network is trained

&lt;div class="keras-header"&gt;&lt;/div&gt;


```python
model.compile(
  loss='mean_squared_error', # Loss function
  optimizer='adam',
  metrics=['mae'] # Optional
)
```
--

The __metrics__ parameter allows to define other measures that could help us to diagnose the model. Those could be for example: `accuracy`, `mae`.

--

All metrics are available &lt;a href = "https://keras.io/metrics/" target = "blank"&gt;here&lt;/a&gt;.

---
class: middle, center
&lt;img src='www/img/nn_workflow_3_eng.png' width = "100%"/&gt;

---
## Training the model

&lt;div class="keras-header"&gt;&lt;/div&gt;


```python
results = model.fit(
  x=mtcars_x, 
  y=mtcars_y, 
  epochs=10
)
```

--

The `epochs` parameter defines the number of iterations. Iteration is __passing through the entire dataset once__.

???

Faktyczny trening (dopasowanie modelu do danych) następuje poprzez wywołanie funkcji `fit()` na obiekcie modelu. Przykładowo:


---
## Visualising the training process

.pull-left[
The training process could be __visualised__ using the `results` object (result of the `fit()` function) containing the history of the training.
]

.pull-right[
&lt;img src='www/img/mtcars_training_vis_python.png' width = "100%"/&gt;
]

---

## Visualising the training process
Here you can find a function that was used to generate the plot.


```r
def plot_training(results, metric = 'loss'):
    plt.plot(results.history[metric])
    if 'val_' + metric in results.history:
      plt.plot(results.history['val_' + metric])
    plt.title('Training history')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    return plt.show()
    
plot_training(results)
```

---
class: inverse

### Exercise 2 (15 min)

1.&amp;nbsp;Define the way the network will be trained.

2.&amp;nbsp;Split the data using the `split_data()` function. The result of this function is a tuple containing the input and output data for both training and test datasets. The input (training) variables should be `latitude` &amp; `longitude`. The output should be `price`.

3.&amp;nbsp; Train the model on 5 epochs using the training dataset and look up the results. 

4.&amp;nbsp; Measure the training time. 

---
## Data split

--

__Machine Learning__ - 80-10-10
--

- 80% for __training__, 
- 10% for __validation__,
- 10% for __testing__.

--

The same split is used in Neural Networks but...

--

If the number of observations is higher that 1M, we can split it 98-1-1.

---
## Batching the data 

In case of large datasets, training the network with all data at once is __often impossible__ because of the hardware limits (the data exceeds RAM).

--

We can solve this using __mini-batching__.

--

This method allows to split the data into smaller subsets, that will be used for training one by one. This way we won't reach the hardware limits. 



---
## Mini-batching

&lt;div class="keras-header"&gt;&lt;/div&gt;


```python
model.fit(
      x=mtcars_x,
      y=mtcars_y,
      epochs=1000,
*     batch_size=2
)
```
--

The `batch_size` parameter indicates the size of the subset. 

--

In case of a dataset with 100 elements `batch_size = 2` will create 50 subsets.

--

The `epochs = 1000` parameter will result in using those 50 subsets 1000 times, meaning that we'll have 50 000 iterations (`1000 * 50`).


---
## Creating a validation set

&lt;div class="keras-header"&gt;&lt;/div&gt;

We can use the `validation_split` parameter in the `fit()` method to split the data into training and validation.

--


```python
model.fit(
      x=mtcars_x,
      y=mtcars_y,
      epochs=1000,
*     validation_split=0.2
)
```


---
class: inverse
### Exercise 3 (5 min)

1.&amp;nbsp;Split the training data further into training (90%) and validation (10%).

2.&amp;nbsp;Train the model on 10 epochs using batches of size 512. 

3.&amp;nbsp;Is the training faster compared to exercise 4?

4.&amp;nbsp; Are the results better?  

---
class: inverse, center, middle

# Let's take a short break!

---
class: inverse
### Exercise 4 (5 min)

Modify the model to include the all of the continuous variables ('latitude', 'longitude', 'number_of_reviews', 'accommodates', 'beds', 'review_scores_rating').

Train the model again.

Are the results better? What are the values of the metrics now?

---
class: inverse, middle, center
# Missing data

---
## Missing data

The neural network __can not accept__ missing data!

--

In case some data is missing, we should:
--

- exclude those observations (not recommended),

--

- apply a __data imputation__ method (for example replace missing values with the average).

---
class: inverse
### Exercise 5 (10 min)

Inspect the data and get rid of missing values where needed.

Train the model again.

How is the model performing now?

---
class: inverse, middle, center

# Linearity of the network

---
## Linearity of the network
.pull-left[
- By default the Neural Network is a combination of linear functions.
]

.pull-right[
&lt;img src='www/img/Warstwy.png' width = "80%"/&gt;
]

---
## Linearity of the network
.pull-left[
- By default the Neural Network is a combination of linear functions.

- Using multiple linear functions __does not differ to much__ from using a single linear function...

]


.pull-right[
&lt;img src='www/img/Warstwy.png' width = "80%"/&gt;
]

---
## Linearity of the network
.pull-left[
- By default the Neural Network is a combination of linear functions.

- Using multiple linear functions __does not differ to much__ from using a single linear function - in both cases the model will search for __linear dependencies__ between variables!

]


.pull-right[
&lt;img src='www/img/Warstwy.png' width = "80%"/&gt;
]


---
## Linearity of the network
.pull-left[
- By default the Neural Network is a combination of linear functions.

- Using multiple linear functions __does not differ to much__ from using a single linear function - in both cases the model will search for __linear dependencies__ between variables!

- However, the data is usually complex and the relationships between variables __nonlinear__.

]


.pull-right[
&lt;img src='www/img/Warstwy.png' width = "80%"/&gt;
]

---
class: inverse, middle, center
# How to make the network nonlinear?

---
class: inverse, middle, center

# Using the activation function!

---
## Activation function

.pull-left[
&lt;img src='www/img/Neuron_input.png' width = "80%"/&gt;
]

.pull-right[
The activation function allows to add __nonlinear transformation__ to the results of the calculation of a single neuron.
]

---
## Activation function

.pull-left[
&lt;img src='www/img/Neuron_input.png' width = "80%"/&gt;
]

.pull-right[
The activation function allows to add __nonlinear transformation__ to the results of the calculation of a single neuron.

The linear function becomes the __argument__ of the activation function `\(a()\)`.

`\(y = a(w_1 * x_1 + w_2 * x_2 + b)\)`
]

---
## Activation function

.pull-left[
&lt;img src='www/img/Neuron_output.png' width = "80%"/&gt;
]

.pull-right[
Calculated value is used by the neurons in the following layers.
]
---
class: inverse, middle, center
# Types of the activation functions

---
## Linear
.absolute-pull-seven[&lt;img src='www/img/linear.png' width = "80%"/&gt;]

.absolute-pull-three[
Typical linear function with values in range of [-Inf, +Inf].

This is the default activation function in Keras.
]

---
## Sigmoid 
.absolute-pull-seven[&lt;img src='www/img/sigmoid.png' width = "80%"/&gt;]

.absolute-pull-three[
Values of the _sigmoid_ function range __from 0 to 1__. 
]

---
## Tanh
.absolute-pull-seven[&lt;img src='www/img/tanh.png' width = "80%"/&gt;]

.absolute-pull-three[
Values of the _tanh_ function (Hyperbolic Tangent) range from __-1 to 1__.
]
---
## ReLU
.absolute-pull-seven[&lt;img src='www/img/relu.png' width = "80%"/&gt;]

.absolute-pull-three[
The _ReLU_ function is one of the __most popular__ activation functions used in neural networks.
]

---
## ReLU
.absolute-pull-seven[&lt;img src='www/img/relu.png' width = "80%"/&gt;]

.absolute-pull-three[
The _ReLu_ function is one of the __most popular__ activation functions used in neural networks.

Using _ReLU_ solves most of the problems caused by _sigmoid_ and _tanh_. Read more &lt;a href = "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" target = "blank"&gt;here&lt;/a&gt;.
]


---
class: inverse, middle, center

# Which activation function to choose?

---
## Activation function and network layers

.pull-left[
&lt;img src='www/img/activation_input.png' width = "80%"/&gt;
]

.pull-right[
In case of the hidden layers, we use functions that works on __each neuron separately__.
]
---
## Activation function and network layers

.pull-left[
&lt;img src='www/img/activation_hidden.png' width = "80%"/&gt;
]

.pull-right[
In case of the hidden layers, we use functions that works on __each neuron separately__.

__ReLU__ is a popular choice.

]

---
## Activation function and network layers

.pull-left[
&lt;img src='www/img/activation_output.png' width = "80%"/&gt;
]

.pull-right[
In case of the hidden layers, we use functions that works on __each neuron separately__.

__ReLU__ is a popular choice.

The activation function on the __output layer__ depends on the __type of problem__ we're trying to solve.
]

---
## Type of output and the activation function

__Binary classification__:

- `sigmoid` - predicted value in range of [0,1]

--

__Multi class classification__:

- `softmax` - The sum of the predicted values equal to one.

--

&amp;nbsp;__Regression__:

- `linear` - We predict a single number on continuous scale.

---
class: inverse, middle, center

# Quiz
### Which activation function to choose on the output layer
---
class: inverse, middle, center

## Detecting spam emails

---
class: inverse, middle, center

## Predicting air temperature

---
class: inverse, middle, center

## Classification of dogs, cats and birds in images


---
class: inverse, middle, center
# The activation function in Keras

---
## Activation function

&lt;div class="keras-header"&gt;&lt;/div&gt;

_Using the Activation layer:_

.super-small-code[

```python
from keras.layers import Dense, Activation

model.add(Dense(1000, input_dim = 3))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
```
]
--


_Using the activation parameter:_

.super-small-code[

```python
model.add(Dense(1000, input_dim = 3, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```
]

---
class: inverse
### Exercise 6 (10 min)

1.&amp;nbsp;Add two additional dense layers of size 128 and 32 to the existing architecture.

--

2.&amp;nbsp;Add proper activation functions.

--

3.&amp;nbsp;Train the model __from scratch__ on 10 epochs. Do you see the difference in results?

---
class: inverse, middle, center
# Data preparation

---
## Data preparation

In comparison to some Machine Learning algorithms, Deep Neural Networks can figure out complex __dependencies between variables__ on it's own.

--

We don't need need to do __feature engineering__ manually! 

--

However, proper data preparation might help to __train significantly faster__ and in some cases reach __better performance__.

---
## Data normalization


&lt;center&gt;&lt;img src='www/img/standaryzacja.png' width = "120%"/&gt;&lt;/center&gt;

---
## Data normalization

.pull-left[
Normalization is an __important step__ for training Neural Networks.
]

--

.pull-right.center.text-middle[
`\({x_i - mean(x) \over std(x)}\)`

&lt;br/&gt;

`\({x_i - min(x) \over max(x) - min(x)}\)`
]


---
## Data normalization

.pull-left[
Normalization is an __important step__ for training Neural Networks.

The data that is not normalized could results in __slow and unstable training__.
]

.pull-right.center.text-middle[
`\({x_i - mean(x) \over std(x)}\)`

&lt;br/&gt;

`\({x_i - min(x) \over max(x) - min(x)}\)`
]

---
## Data normalization

.pull-left[
Normalization is an __important step__ for training Neural Networks.

The data that is not normalized could results in __slow and unstable training__.

In case of a regression problem __scaling__ is __necessary__...
]

.pull-right.center.text-middle[
`\({x_i - mean(x) \over std(x)}\)`

&lt;br/&gt;

`\({x_i - min(x) \over max(x) - min(x)}\)`
]


---
## Data normalization

.pull-left[
Normalization is an __important step__ for training Neural Networks.

The data that is not normalized could results in __slow and unstable training__.

In case of a regression problem __scaling__ is __necessary__...

...If the data is not scaled, the training could fail.
]

.pull-right.center.text-middle[
`\({x_i - mean(x) \over std(x)}\)`

&lt;br/&gt;

`\({x_i - min(x) \over max(x) - min(x)}\)`
]

---
## Data normalization

.pull-left[
If a variable is __normally distributed__, we should standardize.
]

.pull-right.center.text-middle[
`\({x_i - mean(x) \over std(x)}\)`

]

---
## Data normalization

.pull-left[
If a variable is __normally distributed__, we should standardize.

Otherwise we should __normalize__.
]

.pull-right.center.text-middle[
`\({x_i - mean(x) \over std(x)}\)`

`\({x_i - min(x) \over max(x) - min(x)}\)`
]

???

If the quantity values are small (near 0-1) and the distribution is limited (e.g. standard deviation near 1) then perhaps you can get away with no scaling of the data.

---
## How to code standardization?

--

.small-code[

```python
from sklearn.preprocessing import StandardScaler

# Calculating of the mean and std dev.
std_scale = StandardScaler().fit(train_mtcars_x)
```
]

---
## How to code standardization

.small-code[

```python
from sklearn.preprocessing import StandardScaler

# Calculating of the mean and std dev.
std_scale = StandardScaler().fit(train_mtcars_x)

# Scaling the train set
x_train_std = std_scale.transform(train_mtcars_x)
```
]

---
## How to code standardization

.small-code[

```python
from sklearn.preprocessing import StandardScaler

# Calculating of the mean and std dev.
std_scale = StandardScaler().fit(train_mtcars_x)

# Scaling the train set
x_train_std = std_scale.transform(train_mtcars_x)

# Scaling the test set with the scaler based on the training set
x_test_std = std_scale.transform(test_mtcars_x)
```
]

---
## How to code __normalization__

.small-code[

```python
*from sklearn.preprocessing import MinMaxScaler

# Calculating of the mean and std dev.
*norm_scale = MinMaxScaler().fit(train_mtcars_x)

# Scaling the train set
x_train_norm = norm_scale.transform(train_mtcars_x)

# Scaling the test set with the scaler based on the training set
x_test_norm = norm_scale.transform(test_mtcars_x)
```
]

---
class: inverse

### Exercise 7 (10 min)

1.&amp;nbsp;Apply transformation of the input data so that the training is not only possible, but optimal.

--

2.&amp;nbsp;Train the model __from scratch__ on 10 epochs. Is there a difference in performance?

*&amp;nbsp;You can try training on larger number of epochs and see if this improves the results.

---
class: middle, center
&lt;img src='www/img/nn_workflow_4_eng.png' width = "100%"/&gt;

---
## Model evaluation

&lt;div class="keras-header"&gt;&lt;/div&gt;

.text22[
After training the model we need to __verify it's performance__ on the test set.
]

--

.text22[
This can be done using the `evaluate()` method.
]


--

.small-code[

```python
model.evaluate(mtcars_test_x, mtcars_test_y)
# [123.432, 3.54]
```
]

--

.text22[
In order to display __metrics names__ which correspond to these values you should use  `metrics_names` attribute.
]


--

.small-code[

```python
model.metrics_names
# ['loss', 'mean_absolute_error']
```
]

---
class: middle, center
&lt;img src='www/img/nn_workflow_5_eng.png' width = "100%"/&gt;

---
## Making predictions

&lt;div class="keras-header"&gt;&lt;/div&gt;

To generate predictions one can use the `predict()` method.

--


```python
model.predict(mtcars_test_x)
```

--

This will return a list equal to the length of the input dataset.

---
class: inverse
### Exercise 8 (5 min)

Verify the model on the test data.

How different are the values for the metrics on the test set comparing to the metrics on the validation set?

---
## How to build a well working Neural Network

--

The process itself is a series of __experiments__. There is __no recipe__ that will work on every dataset.

--

You can experiment with:

--

- number of layers, neurons,
--

- number of `epochs`,
--

- batch size,
--

- activation functions,
--

- optimization algorithms,
--

- loss functions.

--

It's very important to remember about using the proper activation function for the output layer (depending on the type of a problem).

---
class: inverse, middle, center

# Summary

---
## Summary

- We've just learned what is a Neural Network and __Deep Learning__.

--

- Based on the Keras R library, we've build a __working model__ predicting the property prices.

--

- However, it's good to remember that in practice the benefits of Deep Learning can be seen only when the __data volume is much bigger__ and the __problem itself it more complex__.

--

- Even though, we believe that because of the simplicity of the provided example, we've convinced you that __learning Deep Learning techniques are not as hard as they could be perceived__. 

---
class: center
## Join our meetup group!
&lt;center&gt;
&lt;img src='www/img/iww.png' width = "100%" style = "margin-top:40px"/&gt;

&lt;h3&gt;&lt;a href = "https://www.meetup.com/iDash-workshop-meetups/" target = "_blank"&gt;meetup.com/iDash-workshop-meetups&lt;/a&gt;&lt;/h3&gt;
&lt;/center&gt;

---
class: center, middle
## Survey
   
   
# http://bit.do/fkyUS

---
class: inverse, bottom, center
background-image: url('www/img/logo_white.png')
background-size: 50%

mb@idash.pl mo@idash.pl
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
